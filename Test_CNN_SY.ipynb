{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ab76dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ea7b6656-3f84-4eb3-9099-23e623fc1018</td>\n",
       "      <td>Top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa</td>\n",
       "      <td>Top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3b86d877-2b9e-4c8b-a6a2-1d87513309d0</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5d3a1404-697f-479f-9090-c1ecd0413d27</td>\n",
       "      <td>Bottoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b0c03127-9dfb-4573-8934-1958396937bf</td>\n",
       "      <td>Top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>3855ea22-5e7f-411f-b1fa-6db27a676c06</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>dfd4079d-967b-4b3e-8574-fbac11b58103</td>\n",
       "      <td>Bottoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>5379356a-40ee-4890-b416-2336a7d84061</td>\n",
       "      <td>Bottoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>65507fb8-3456-4c15-b53e-d1b03bf71a59</td>\n",
       "      <td>Shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>32b99302-cec7-4dec-adfa-3d4029674209</td>\n",
       "      <td>Bottoms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4677 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image    label\n",
       "0     ea7b6656-3f84-4eb3-9099-23e623fc1018      Top\n",
       "1     ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa      Top\n",
       "2     3b86d877-2b9e-4c8b-a6a2-1d87513309d0    Shoes\n",
       "3     5d3a1404-697f-479f-9090-c1ecd0413d27  Bottoms\n",
       "4     b0c03127-9dfb-4573-8934-1958396937bf      Top\n",
       "...                                    ...      ...\n",
       "4672  3855ea22-5e7f-411f-b1fa-6db27a676c06    Shoes\n",
       "4673  dfd4079d-967b-4b3e-8574-fbac11b58103  Bottoms\n",
       "4674  5379356a-40ee-4890-b416-2336a7d84061  Bottoms\n",
       "4675  65507fb8-3456-4c15-b53e-d1b03bf71a59    Shoes\n",
       "4676  32b99302-cec7-4dec-adfa-3d4029674209  Bottoms\n",
       "\n",
       "[4677 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('images_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a415f90",
   "metadata": {},
   "source": [
    "# Encode labels, take sample of data into train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93fa1093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, ..., 0, 2, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df.label)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a27a5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9916e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.1)\n",
    "for _, group_index in ss.split(images, labels):\n",
    "    group_x = images[group_index].to_numpy()\n",
    "    group_y = labels[group_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78eb944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "for train_index, test_index in ss.split(group_x, group_y):\n",
    "    train_x, test_x = group_x[train_index], group_x[test_index]\n",
    "    train_y, test_y = group_y[train_index], group_y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62edb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "for train_index, valid_index in ss.split(train_x, train_y):\n",
    "    train_x, valid_x = train_x[train_index], train_x[valid_index]\n",
    "    train_y, valid_y = train_y[train_index], train_y[valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f2438",
   "metadata": {},
   "source": [
    "# Get all images loaded in from sample and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddbd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def get_images(path_list):\n",
    "    train_img = []\n",
    "    for filename in path_list:\n",
    "        path = 'images_compressed/' + filename + '.jpg'\n",
    "        img = imread(path, as_gray=True)\n",
    "        img = resize(img, (100, 100))\n",
    "        img /= 255.0\n",
    "        img = img.astype('float32')\n",
    "        train_img.append(img)\n",
    "    return np.asarray(train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3c1d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_images = get_images(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b3113b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(299, 100, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7170de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x_images = get_images(valid_x)\n",
    "test_x_images = get_images(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03257ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_x_torch = torch.from_numpy(train_x_images).unsqueeze(dim=1)\n",
    "valid_x_torch = torch.from_numpy(valid_x_images).unsqueeze(dim=1)\n",
    "test_x_torch = torch.from_numpy(test_x_images).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "788aa0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([299, 1, 100, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0f31717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([299])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_torch = torch.from_numpy(train_y)\n",
    "valid_y_torch = torch.from_numpy(valid_y)\n",
    "test_y_torch = torch.from_numpy(test_y)\n",
    "train_y_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bda2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available\n",
    "if torch.cuda.is_available():\n",
    "    train_x_torch = train_x_torch.cuda()\n",
    "    train_y_torch = train_y_torch.cuda()\n",
    "    valid_x_torch = valid_x_torch.cuda()\n",
    "    valid_y_torch = valid_y_torch.cuda()\n",
    "    test_x_torch = test_x_torch.cuda()\n",
    "    test_y_torch = test_y_torch.cuda()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08cc17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(zip(train_x_torch, train_y_torch.to(dtype=torch.long)))\n",
    "test = list(zip(test_x_torch, test_y_torch.to(dtype=torch.long)))\n",
    "valid = list(zip(valid_x_torch, valid_y_torch.to(dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "636cc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put everything into a data loader because too much data for gpu\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(train, batch_size=16)\n",
    "testloader = DataLoader(test, batch_size=16)\n",
    "validloader = DataLoader(test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b1cb5",
   "metadata": {},
   "source": [
    "# Create CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cab59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# need to figure out what the numbers in Conv2d and other parts are / what to use for them\n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNN, self).__init__()\n",
    "        \n",
    "        # two layers for this cnn\n",
    "        self.cnn_layers = nn.Sequential (\n",
    "            # Defining a 2D convolution layer\n",
    "            nn.Conv2d(1, 32, kernel_size=9, stride=1, padding=1),   # output = 94 x 94 x 32\n",
    "            nn.BatchNorm2d(32),                                     # same shape\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                  # output = 47 X 47 X 32\n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(32, 32, kernel_size=9, stride=1, padding=1),  # output = 41 X 41 X 32\n",
    "            nn.BatchNorm2d(32),                                     # same shape\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                  # output = 20 X 20 X 32\n",
    "      \n",
    "        )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 20 * 20, 4)\n",
    "        )\n",
    "        \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bfc6252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNN(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 32, kernel_size=(9, 9), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=12800, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "model = ConvNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00952669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0 loss = 1.1 train acc= 0.625 valid loss = 0.902 valid acc = 0.66\n",
      "At epoch 1 loss = 0.685 train acc= 0.759 valid loss = 0.86 valid acc = 0.691\n",
      "At epoch 2 loss = 0.532 train acc= 0.803 valid loss = 0.815 valid acc = 0.691\n",
      "At epoch 3 loss = 0.417 train acc= 0.849 valid loss = 0.798 valid acc = 0.691\n",
      "At epoch 4 loss = 0.329 train acc= 0.883 valid loss = 0.825 valid acc = 0.691\n",
      "At epoch 5 loss = 0.261 train acc= 0.91 valid loss = 0.874 valid acc = 0.681\n",
      "At epoch 6 loss = 0.211 train acc= 0.926 valid loss = 0.931 valid acc = 0.67\n",
      "At epoch 7 loss = 0.183 train acc= 0.946 valid loss = 0.905 valid acc = 0.691\n",
      "At epoch 8 loss = 0.154 train acc= 0.943 valid loss = 0.802 valid acc = 0.723\n",
      "At epoch 9 loss = 0.114 train acc= 0.97 valid loss = 0.75 valid acc = 0.777\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        for i in range(len(labels)):\n",
    "            image = images[i].view(1, 1, 100, 100)\n",
    "            with torch.no_grad():\n",
    "                logps = model(image)\n",
    "            allprobs = torch.exp(logps)\n",
    "            prob = allprobs.tolist()[0]\n",
    "            hyp = prob.index(max(prob))\n",
    "            train_y = labels[i]\n",
    "            train_total += 1\n",
    "            if train_y.item() == hyp:\n",
    "                train_correct += 1\n",
    "    for images, labels in validloader:\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        valid_loss += loss.item()\n",
    "        for i in range(len(labels)):\n",
    "            image = images[i].view(1, 1, 100, 100)\n",
    "            with torch.no_grad():\n",
    "                logps = model(image)\n",
    "            allprobs = torch.exp(logps)\n",
    "            prob = allprobs.tolist()[0]\n",
    "            hyp = prob.index(max(prob))\n",
    "            valid_y = labels[i]\n",
    "            valid_total += 1\n",
    "            if valid_y.item() == hyp:\n",
    "                valid_correct += 1\n",
    "    print('At epoch', epoch, 'loss =', '%.3g'%(train_loss/len(trainloader)), 'train acc=', '%.3g'%(train_correct/train_total), 'valid loss =', '%.3g'%(valid_loss/len(validloader)), 'valid acc =', '%.3g'%(valid_correct/valid_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776595744680851\n"
     ]
    }
   ],
   "source": [
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "hyps = []\n",
    "correct_labels = []\n",
    "\n",
    "for images, labels in testloader:\n",
    "    for i in range(len(labels)):\n",
    "        image = images[i].view(1, 1, 100, 100)\n",
    "        with torch.no_grad():\n",
    "            logps = model(image)\n",
    "        allprobs = torch.exp(logps)\n",
    "        prob = allprobs.tolist()[0]\n",
    "        hyp = prob.index(max(prob))\n",
    "        hyps.append(hyp)\n",
    "        test_y = labels[i]\n",
    "        total += 1\n",
    "        correct_labels.append(test_y.item())\n",
    "        if test_y.item() == hyp:\n",
    "            correct += 1\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9a00344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  0,  0,  6],\n",
       "       [ 2,  0,  0,  1],\n",
       "       [ 1,  0,  2,  6],\n",
       "       [ 4,  0,  1, 54]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(correct_labels, hyps)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "thorough-desktop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bottoms       0.85      0.89      0.87       231\n",
      "        Hats       0.59      0.47      0.52        34\n",
      "       Shoes       0.81      0.64      0.72        87\n",
      "         Top       0.92      0.94      0.93       584\n",
      "\n",
      "    accuracy                           0.89       936\n",
      "   macro avg       0.79      0.74      0.76       936\n",
      "weighted avg       0.88      0.89      0.88       936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(correct_labels, hyps, target_names=['Bottoms', 'Hats', 'Shoes', 'Top']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-czech",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
